# ## 题目

###简述 K- means 算法的关键步骤，并分析其复杂性与优缺点。

K均值聚类的核心目标是将给定的数据集划分成K个簇，并给出每个数据对应的簇中心点。算法的具体步骤描述如下：
- 1.数据预处理，如归一化，离群点处理等。
- 2.随机选取K个簇中心，记为 $u_1,u_2,...,u_k$,
- 3.定义代价函数，$J(c,u)=\sum_{i=1}^C\sum_{j=1}^N\Vert x_j-c_i\Vert^2 $
- 4.令t=0,1,2...为迭代步数，重复下面过程直到J收敛
- 对于每一个样本x，将其分配到距离最近的簇
- 对于每一个类簇K，重新计算该类簇的中心

优点：
- 容易理解，聚类效果不错，虽然是局部最优， 但往往局部最优就够了；
- 处理大数据集的时候，该算法可以保证较好的伸缩性；
- 当簇近似高斯分布的时候，效果非常不错；
- 算法复杂度低。

缺点：
- K 值需要人为设定，不同 K 值得到的结果不一样；
- 对初始的簇中心敏感，不同选取方式会得到不同结果；
- 对异常值敏感；
- 样本只能归为一类，不适合多分类任务；
- 不适合太离散的分类、样本类别不平衡的分类、非凸形状的分类。

时间复杂度：$O(tknm)$ ，其中，t 为迭代次数，k 为簇的数目，n 为样本点数，m 为样本点维度。
空间复杂度： $O(m(n+k))$  ，其中，k 为簇的数目，m 为样本点维度，n 为样本点数。


###设某公路上经过的货车与客车的数量之比为2:1，货车中途停车修理的概率为0.02，客车为0.01，今有一辆汽车中途停车修理，求该汽车是货车的概率
![](media/16100177096209/16100321396609.jpg)


###简述最近邻分类器、k近邻分类器、支撑向量机（SVM）、主成分分析（PCA）、线性判别分析（LDA）这五种方法的分类思想

- 最近邻分类器：计算测试样本与所有样本的距离，将测试样本归为距离最近的样本类。
- k近邻分类器:计算测试样本与K个最近样本的距离，将测试样本归为K个样本中相同类别个数较多的一类。
- SVM：在高或无限维度空间中构造超平面或超平面，可用于分类、回归或其他任务。使超平面与任何类最近的训练数据点之间的距离最大，因为一般来说，距离边界越大，分类器的泛化误差就越低。
- PCA:将高维的数据通过线性变换投影到低维空间上去，但这个投影可不是随便投投，要遵循一个指导思想，那就是：找出最能够代表原始数据的投影方法。投影到方差最大的几个相互正交的方向上，以期待保留最多的样本信息，样本的方差越大表示样本的多样性越好。
- LDA：将带有标签的数据降维，投影到低维空间同时满足三个条件：1，尽可能多的保留数据样本的信息（即选择最大的特征是对应的特征向量所代表的方向）。2，寻找使样本尽可能好分的最佳投影方向。3，投影后使得同类样本尽可能近，不同类样本尽可能远。(投影后类内方差最小，类间方差最大)


###四、局部线性嵌入算法（Locally Linear Embedding, LLE）在处理流形降的时考虑样本点局部信息，效果比 PCA 要好很多。对于每个数据点 x，用 k 近邻算法获得它的 k 近邻点

![](media/16100177096209/16100345687178.jpg)
###PCA 协方差矩阵

![](media/16100177096209/16100380215765.jpg)![2381610038029_.pi](media/16100177096209/2381610038029_.pic.jpg)

![](media/16100177096209/16100740835537.jpg)![196601610074117_.pi](media/16100177096209/196601610074117_.pic.jpg)

![人工智能技术 - 复习资料](media/16100741932730/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%8A%80%E6%9C%AF%20-%20%E5%A4%8D%E4%B9%A0%E8%B5%84%E6%96%99.png)
